{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with Telco Churn data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build a ML model for telco churn prediction. \n",
    "The data used is from this kaggle data set: https://www.kaggle.com/blastchar/telco-customer-churn\n",
    "As this notebook is part of a bigger workshop We assume that you already have this data loaded on a gs://{project_id} location\n",
    "\n",
    "We are going to build a basic model using sklearn. The purpose is to experiment with feature engineering and try few models to pick the best. \n",
    "Once we are happy with the processing and model building parts of our code, we are going to create a package of our code and execute this on AI platform training service.\n",
    "\n",
    "!The packageing and execution to AI Platform is not part of this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules needed for our ML process\n",
    "\n",
    "#import google.auth\n",
    "import dask.dataframe as dd\n",
    "from google.cloud import bigquery, bigquery_storage\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt  \n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import Union, List\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set the PROJECT variable to our project id. The project id can be found on your GCP console but the following automates the process by getting the value from !gcloud config get-value project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT=!gcloud config get-value project # returns default project id \n",
    "PROJECT=PROJECT[0]\n",
    "\n",
    "BUCKET = \"gs://\"+PROJECT \n",
    "\n",
    "DATA_GCS_LOCATION = BUCKET+\"/telco-churn/data/data_v1.csv\"\n",
    "\n",
    "JOB_TIMESTAMP = datetime.now().strftime('%Y%m%d-%H%M%S') # Creating timestamp so that every execution of the notebook writes to a unique gs location\n",
    "\n",
    "MODEL_DIR = BUCKET+\"/telco-churn/experiment-\"+JOB_TIMESTAMP # adding the timestamp to the model_dir path.\n",
    "                                                        # This path is used to write assets like the trained model and job report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below 2 cells create a dataset on AI platform. As the sourse of the data will not change **you will only need to run this once**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from Google Cloud Storage (GCS) to Pandas Dataframe\n",
    "\n",
    "In the following cell we are pulling data from GCS and loading them to a dataframe. Keep in mind that data might not fit your instance memory and therefore we might need to only bring a sample of the data. That is not a big problem as we are only experimenting. When we will be running our training job on AI Platform training we need to pick the right instance with enough memory.\n",
    "\n",
    "Additionally our telco dataset fits the memory so we will go ahead and load everything.\n",
    "\n",
    "Finally we will be using Dask (https://docs.dask.org/en/latest/) to load our data to Pandas. Dask is a library that allows parallelism of varius operations like data processing and ML training. In our case we want to leverage the ability of loading data using a wild card. If your data are in multible files wild cards allows you to define the the singniture of the data file path and load multiple files. \n",
    "\n",
    "for example if you have:\n",
    "* gs://my_bucket/training-data/part-1.csv\n",
    "* gs://my_bucket/training-data/part-2.csv\n",
    "* gs://my_bucket/training-data/part-3.csv\n",
    "    \n",
    "you can use the following wild card to load all:\n",
    "*  gs://my_bucket/training-data/part-*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_gcs(data_gcs_path: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Loads data from Google Cloud Storage (GCS) to a dataframe\n",
    "\n",
    "            Parameters:\n",
    "                    data_gcs_path (str): gs path for the location of the data. Wildcards are also supported. i.e gs://example_bucket/data/training-*.csv\n",
    "\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
    "    '''\n",
    "        \n",
    "    # using dask that supports wildcards to read multiple files. Then with dd.read_csv().compute we create a pandas dataframe\n",
    "    # Additionally I have noticed that some values for TotalCharges are missing and this creates confusion regarding TotalCharges the data types. \n",
    "    # to overcome this we manually define TotalCharges as object. \n",
    "    # We will later fix this upnormality\n",
    "    logging.info(\"reading gs data: {}\".format(data_gcs_path))\n",
    "    return dd.read_csv(data_gcs_path, dtype={'TotalCharges': 'object'}).compute()\n",
    "\n",
    "# This is not used at this tutorial but it is here to demonstrate how loading data directly from big query looks like\n",
    "def load_data_from_bq(bq_uri: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Loads data from BigQuery table (BQ) to a dataframe\n",
    "\n",
    "            Parameters:\n",
    "                    bq_uri (str): bq table uri. i.e: example_project.example_dataset.example_table\n",
    "            Returns:\n",
    "                    pandas.DataFrame: a dataframe with the data from GCP loaded\n",
    "    '''\n",
    "    \n",
    "    logging.info(\"reading bq data: {}\".format(bq_uri))\n",
    "    project,dataset,table =  bq_uri.split(\".\")\n",
    "    bqclient = bigquery.Client(project=PROJECT)\n",
    "    bqstorageclient = bigquery_storage.BigQueryReadClient()\n",
    "    query_string = \"\"\"\n",
    "    SELECT * from {ds}.{tbl}\n",
    "    \"\"\".format(ds=dataset, tbl=table)\n",
    "\n",
    "    return (\n",
    "        bqclient.query(query_string)\n",
    "        .result()\n",
    "        .to_dataframe(bqstorage_client=bqstorageclient)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data_from_gcs(DATA_GCS_LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how the data loaded in the dataframe look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns) # Print all columns in the dataframe\n",
    "df.head(4) # Show my the first 4 records of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Features Types\n",
    "Define the different features types you have in the following categories. The feature names need to match the column names in your CSV file.\n",
    "Do not include any columns that you want to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature categories\n",
    "\n",
    "# List all binary features: 0,1 or True,Fales or Male,Female etc\n",
    "BINARY_FEATURES = [\n",
    "    'gender',\n",
    "    'SeniorCitizen',\n",
    "    'Partner',\n",
    "    'Dependents',\n",
    "    'PhoneService',\n",
    "    'PaperlessBilling']\n",
    "\n",
    "# List all numeric features\n",
    "NUMERIC_FEATURES = [\n",
    "    'tenure',\n",
    "    'MonthlyCharges',\n",
    "    'TotalCharges']\n",
    "\n",
    "# List all categorical features \n",
    "CATEGORICAL_FEATURES = [\n",
    "    'InternetService',\n",
    "    'OnlineSecurity',\n",
    "    'DeviceProtection',\n",
    "    'TechSupport',\n",
    "    'StreamingTV',\n",
    "    'StreamingMovies',\n",
    "    'Contract',\n",
    "    'PaymentMethod',\n",
    "    'MultipleLines']\n",
    "\n",
    "ALL_COLUMNS = BINARY_FEATURES+NUMERIC_FEATURES+CATEGORICAL_FEATURES\n",
    "\n",
    "LABEL = 'Churn'\n",
    "\n",
    "# We define the index position of each feature. This will be needed when we wil be processing a \n",
    "# numpy array (instead of pandas) that has no column names.\n",
    "BINARY_FEATURES_IDX = list(range(0,len(BINARY_FEATURES)))\n",
    "NUMERIC_FEATURES_IDX = list(range(len(BINARY_FEATURES), len(BINARY_FEATURES)+len(NUMERIC_FEATURES)))\n",
    "CATEGORICAL_FEATURES_IDX = list(range(len(BINARY_FEATURES+NUMERIC_FEATURES), len(ALL_COLUMNS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are some invalid values in TotalCharges column where the TotalCharges is missing. Look at the first record below, data are order based on TotalCharges.\n",
    "\n",
    "Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"TotalCharges\", ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hm... I suspect that the reason is that new customers do not have TotalCharges as this is their first month.\n",
    "\n",
    "We can evaluate this by looking into the min and max vlaues for each tenure. \n",
    "\n",
    "As suspected none of 0 tenure datapoints have TotalCharges set yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = df.groupby(['tenure'])\n",
    "gb['TotalCharges'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected none of 0 tenure datapoints have TotalCharges set yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['tenure']==0, ['tenure', 'MonthlyCharges', 'TotalCharges']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okey lets fix this by assigning the values of this column to equal the MonthlyCharges. For new customers at the end of the first month the total charges should be the same as that months' charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that \n",
    "def sort_missing_total_charges(df: pd.DataFrame):\n",
    "    '''\n",
    "    Alters the received dataframe and sets missing TotalChanges \n",
    "    equal to MonthlyCharges when tenure is 0.\n",
    "\n",
    "            Parameters:\n",
    "                    df (pandas.DataFrame): The Pandas Dataframe to alter\n",
    "            Returns:\n",
    "                    None\n",
    "    '''\n",
    "    df.loc[df.tenure == 0, 'TotalCharges'] = df.loc[df.tenure == 0, 'MonthlyCharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_missing_total_charges(df)\n",
    "\n",
    "# Is the problem now solved?\n",
    "df.loc[df.tenure==0, ['tenure', 'MonthlyCharges', 'TotalCharges']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want all our columns. customerID is unique for every customer and does not provide any information for our model.\n",
    "\n",
    "Additionaly we want to separate the label column from the training data.\n",
    "\n",
    "It is important to notice that we are also reordering the columns, Binary first, then the numeric and then the categorical ones.\n",
    "\n",
    "This is the order that the model will be accepting the features for prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_selection(df: pd.DataFrame, selected_columns: List[str], label_column: str) -> (pd.DataFrame, pd.Series):\n",
    "    '''\n",
    "    From a dataframe create a new dataframe with only selected columns and returns it.\n",
    "    Additionally it splits the label column into a pandas Series.\n",
    "\n",
    "            Parameters:\n",
    "                    df (pandas.DataFrame): The Pandas Dataframe to drop columns and extract label\n",
    "                    selected_columns (List[str]): List of strings with the selected columns. i,e ['col_1', 'col_2', ..., 'col_n' ]\n",
    "                    label_column (str): The name of the label column\n",
    "\n",
    "            Returns:\n",
    "                    tuple(pandas.DataFrame, pandas.Series): Tuble with the new pandas DataFrame containing only selected columns and lablel pandas Series\n",
    "    '''\n",
    "    # We create a series with the prediciton label\n",
    "    labels = df[label_column]\n",
    "    \n",
    "    data = df.loc[:, selected_columns]\n",
    "    \n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_selection(df, ALL_COLUMNS, LABEL);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our data sto training and test sets and also select only the columns that we need to use in our training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We have columns in multible formats. Some are numerical, some are binary categorical having only 2 values, and some are categorical with multiple options.\n",
    "We are going to use StandardScaler for the numeric features, OrdinalEncoder for the binary ones and OneHotEncoder for the multiple categories. \n",
    "\n",
    "Additionally, in the pipeline builder appart from defining the feature engineering transformations, we will also defin the model and its parameters. we will pack all together in a pipeline and return that.\n",
    "\n",
    "Our pipeline will look like this (well.. in reality the first 3 steps for feature engineering will run in parallel because we defined n_jobs=-1)\n",
    "\n",
    "-> OrdinalEncoder() -> OrdinalEncoder() -> OrdinalEncoder() -> SVC() ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_builder(params_svm: dict, bin_ftr_idx: List[int], num_ftr_idx: List[int], cat_ftr_idx: List[int]) -> Pipeline:\n",
    "    '''\n",
    "    Builds a sklearn pipeline with preprocessing and model configuration.\n",
    "    Preprocessing steps are:\n",
    "        * OrdinalEncoder - used for binary features\n",
    "        * StandardScaler - used for numerical features\n",
    "        * OneHotEncoder - used for categorical features\n",
    "    Model used is SVC\n",
    "\n",
    "            Parameters:\n",
    "                    params_svm (dict): List of parameters for the sklearn.svm.SVC classifier \n",
    "                    bin_ftr_idx (List[str]): List of ints that mark the column indexes with binary columns. i.e [0, 2, ... , X ]\n",
    "                    num_ftr_idx (List[str]): List of ints that mark the column indexes with numerica columns. i.e [6, 3, ... , X ]\n",
    "                    cat_ftr_idx (List[str]): List of ints that mark the column indexes with categorical columns. i.e [5, 10, ... , X ]\n",
    "                    label_column (str): The name of the label column\n",
    "\n",
    "            Returns:\n",
    "                     Pipeline: sklearn.pipelines.Pipeline with preprocessing and model training\n",
    "    '''\n",
    "        \n",
    "    # Definining a preprocessing step for our pipeline. \n",
    "    # it specifies how the features are going to be transformed\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('bin', OrdinalEncoder(), BINARY_FEATURES_IDX),\n",
    "            ('num', StandardScaler(), NUMERIC_FEATURES_IDX),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES_IDX)], n_jobs=-1)\n",
    "\n",
    "\n",
    "    # We now create a full pipeline, for preprocessing and training.\n",
    "    # for training we selected a linear SVM classifier\n",
    "    \n",
    "    clf = SVC()\n",
    "    clf.set_params(**params_svm)\n",
    "    \n",
    "    return Pipeline(steps=[ ('preprocessor', preprocessor),\n",
    "                          ('classifier', clf)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split our data to  80% training and 20% test sets, and we will create our training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\"kernel\":\"linear\", \"C\":2, \"class_weight\":None}\n",
    "clf = pipeline_builder(model_params, BINARY_FEATURES_IDX, NUMERIC_FEATURES_IDX, CATEGORICAL_FEATURES_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ML model\n",
    "In the next step we are going to train our model and predict on the test data. We will then use the predictions to evaluate our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipeline(clf: Pipeline, X: Union[pd.DataFrame, np.ndarray], y: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "    '''\n",
    "    Trains a sklearn pipeline by fiting training data an labels and returns the accuracy f1 score\n",
    "    \n",
    "            Parameters:\n",
    "                    clf (sklearn.pipelines.Pipeline): the Pipeline object to fit the data\n",
    "                    X: (pd.DataFrame OR np.ndarray): Training vectors of shape n_samples x n_features, where n_samples is the number of samples and n_features is the number of features.\n",
    "                    y: (pd.DataFrame OR np.ndarray): Labels of shape n_samples. Order should mathc Training Vectors X\n",
    "\n",
    "            Returns:\n",
    "                    score (float): Average F1 score from all cross validations\n",
    "    '''\n",
    "    # run cross validation to get training score. we can use this score to optimise training\n",
    "    score = cross_val_score(clf, X, y, cv=10, n_jobs=-1).mean()\n",
    "    \n",
    "    # Now we fit all our data to the classifier. Shame to leave a portion of the data behind\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = train_pipeline(clf, X_train, y_train)\n",
    "print(\"the prediction f1 score average during cross validation is {}\".format(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model to Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to export our model to Gloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gcs_uri(uri: str) -> (str, str, str, str):\n",
    "    '''\n",
    "    Receives a Google Cloud Storage (GCS) uri and breaks it down to the sheme, bucket, path and file\n",
    "    \n",
    "            Parameters:\n",
    "                    uri (str): GCS uri\n",
    "\n",
    "            Returns:\n",
    "                    scheme (str): uri scheme\n",
    "                    bucket (str): uri bucket\n",
    "                    path (str): uri path\n",
    "                    file (str): uri file\n",
    "    '''\n",
    "    url_arr = uri.split(\"/\")\n",
    "    if \".\" not in url_arr[-1]:\n",
    "        file = \"\"\n",
    "    else:\n",
    "        file = url_arr.pop()\n",
    "    scheme = url_arr[0]\n",
    "    bucket = url_arr[2]\n",
    "    path = \"/\".join(url_arr[3:])\n",
    "    path = path[:-1] if path.endswith(\"/\") else path\n",
    "    \n",
    "    return scheme, bucket, path, file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_export_gcs(fitted_pipeline: Pipeline, model_dir: str) -> str:\n",
    "    '''\n",
    "    Exports trained pipeline to GCS\n",
    "    \n",
    "            Parameters:\n",
    "                    fitted_pipeline (sklearn.pipelines.Pipeline): the Pipeline object with data already fitted (trained pipeline object)\n",
    "                    model_dir (str): GCS path to store the trained pipeline. i.e gs://example_bucket/training-job\n",
    "            Returns:\n",
    "                    export_path (str): Model GCS location\n",
    "    '''\n",
    "    scheme, bucket, path, file = process_gcs_uri(model_dir)\n",
    "    if scheme != \"gs:\":\n",
    "            raise ValueError(\"URI scheme must be gs\")\n",
    "    \n",
    "    # Upload the model to GCS\n",
    "    b = storage.Client().bucket(bucket)\n",
    "    export_path = os.path.join(path, 'model.pkl')\n",
    "    blob = b.blob(export_path)\n",
    "    \n",
    "    blob.upload_from_string(pickle.dumps(fitted_pipeline))\n",
    "    return scheme + \"//\" + os.path.join(bucket, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_export_gcs(clf, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model\n",
    "What do you think of this model? Is it accurate enough? Shall we push it to production?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(\"\\n Confusion Matrix\")\n",
    "plot_confusion_matrix(clf, X_test,y_test, normalize='true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model without a report is as good as a teleportation machine without a manual.. \n",
    "\n",
    "You should not try to use it! \n",
    "\n",
    "We must prepare a report and save it with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_report(cv_score: float, model_params: dict, classification_report: str, columns: List[str], example_data: np.ndarray) -> str:\n",
    "    '''\n",
    "    Prepares a training job repor in Text\n",
    "    \n",
    "            Parameters:\n",
    "                    cv_score (float): score of the training job during cross validation of training data\n",
    "                    model_params (dict): dictonary containing the parameters the model was trained with\n",
    "                    classification_report (str): Model classification report with test data\n",
    "                    columns (List[str]): List of columns that where used in training.\n",
    "                    example_data (np.array): Sample of data (2-3 rows are enough). This is used to include what the prediciton payload should look like for the model\n",
    "            Returns:\n",
    "                    report (str): Full report in text\n",
    "    '''\n",
    "    \n",
    "    buffer_example_data = '['\n",
    "    for r in example_data:\n",
    "        buffer_example_data+='['\n",
    "        for c in r:\n",
    "            if(isinstance(c,str)):\n",
    "                buffer_example_data+=\"'\"+c+\"', \"\n",
    "            else:\n",
    "                buffer_example_data+=str(c)+\", \"\n",
    "        buffer_example_data= buffer_example_data[:-2]+\"], \\n\"\n",
    "    buffer_example_data= buffer_example_data[:-3]+\"]\"\n",
    "        \n",
    "    report = \"\"\"\n",
    "Training Job Report    \n",
    "    \n",
    "Cross Validation Score: {cv_score}\n",
    "\n",
    "Training Model Parameters: {model_params}\n",
    "    \n",
    "Test Data Classification Report:\n",
    "{classification_report}\n",
    "\n",
    "Example of data array for prediciton:\n",
    "\n",
    "Order of columns:\n",
    "{columns}\n",
    "\n",
    "Example for clf.predict()\n",
    "{predict_example}\n",
    "\n",
    "\n",
    "Example of GCP API request body:\n",
    "{{\n",
    "    \"instances\": {json_example}\n",
    "}}\n",
    "\n",
    "\"\"\".format(\n",
    "    cv_score=cv_score,\n",
    "    model_params=json.dumps(model_params),\n",
    "    classification_report=classification_report,\n",
    "    columns = columns,\n",
    "    predict_example = buffer_example_data,\n",
    "    json_example = json.dumps(example_data.tolist()))\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_export_gcs(report: str, report_dir: str) -> None:\n",
    "    '''\n",
    "    Exports training job report to GCS\n",
    "    \n",
    "            Parameters:\n",
    "                    report (str): Full report in text to sent to GCS\n",
    "                    report_dir (str): GCS path to store the report model. i.e gs://example_bucket/training-job\n",
    "            Returns:\n",
    "                    export_path (str): Report GCS location\n",
    "    '''\n",
    "    scheme, bucket, path, file = process_gcs_uri(report_dir)\n",
    "    if scheme != \"gs:\":\n",
    "            raise ValueError(\"URI scheme must be gs\")\n",
    "            \n",
    "    # Upload the model to GCS\n",
    "    b = storage.Client().bucket(bucket)\n",
    "    \n",
    "    export_path = os.path.join(path, 'report.pkl')\n",
    "    blob = b.blob(export_path)\n",
    "    \n",
    "    blob.upload_from_string(report)\n",
    "    \n",
    "    return scheme + \"//\" + os.path.join(bucket, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = prepare_report(cv_score,\n",
    "                        model_params,\n",
    "                        classification_report(y_test,y_pred),\n",
    "                        ALL_COLUMNS, \n",
    "                        X_test.to_numpy()[0:2])\n",
    "\n",
    "report_export_gcs(report, MODEL_DIR)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pause... Reflect... and...\n",
    "### GO BACK TO QWIKLABS :)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
